package main

import (
	"context"
	"encoding/json"
	"errors"
	"log"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"github.com/montanaflynn/stats" // ç”¨æ–¼ P99 çµ±è¨ˆ
	"github.com/segmentio/kafka-go"
)

// è¨Šæ¯çµæ§‹ï¼Œå°æ‡‰ Producer ç™¼é€çš„ JSON
type SensorData struct {
	Timestamp float64 `json:"timestamp"` // Producer å‚³é€æ™‚é–“ (ç§’ç´š)
	DeviceID  string  `json:"device_id"`
	Value     float64 `json:"value"`
}

// --- é…ç½®åƒæ•¸ ---
const (
	KAFKA_BROKER = "localhost:9092"
	TOPIC        = "test-data"
	GROUP_ID     = "go-consumer"
	WORKER_COUNT = 8    // 8 å€‹ Goroutine è² è²¬ JSON è§£æå’Œè¨ˆç®—
	BATCH_SIZE   = 1000 // çµ±è¨ˆæ‰¹æ¬¡å¤§å°
)

// --- è™•ç†å™¨ (Worker) ---
// è² è²¬è§£æ JSONã€è¨ˆç®—å»¶é²ï¼Œä¸¦å°‡å»¶é²çµæœç™¼é€çµ¦çµ±è¨ˆå™¨
func worker(id int, msgs <-chan []byte, results chan<- float64) {
	var data SensorData
	for msgBytes := range msgs {
		recvTimeNano := time.Now().UnixNano() // Go æ¥æ”¶æ™‚é–“ (ç´ç§’)

		// 1. JSON ååºåˆ—åŒ– (CPU å¯†é›†)
		if err := json.Unmarshal(msgBytes, &data); err != nil {
			log.Printf("Worker %d: JSON Unmarshal error: %v", id, err)
			continue
		}

		// 2. è¨ˆç®—å»¶é² (ä½¿ç”¨ç´ç§’ç´šç²¾åº¦)
		// Producer ç™¼é€æ™‚é–“ (ç§’ç´š float) * 10^9 è½‰ç‚ºç´ç§’ç´šæ•´æ•¸
		sendTimeNano := int64(data.Timestamp * 1e9)
		latencyNano := recvTimeNano - sendTimeNano

		// å°‡å»¶é²çµæœ (ç§’ç´š float) ç™¼é€çµ¦çµ±è¨ˆå™¨
		results <- float64(latencyNano) / 1e9
	}
	log.Printf("Worker %d: Shutting down...", id)
}

// --- çµ±è¨ˆå™¨ (Stats Aggregator) ---
// è² è²¬æ”¶é›†å»¶é²æ•¸æ“šã€è¨ˆç®—ä¸¦å®šæœŸè¼¸å‡ºååé‡å’Œ P99 å»¶é²
func statsAggregator(results <-chan float64) {
	var latencies []float64
	var totalProcessed int64 = 0
	startTime := time.Now()

	log.Println("Stats Aggregator started.")

	for latency := range results {
		latencies = append(latencies, latency)
		totalProcessed++

		if len(latencies) >= BATCH_SIZE {
			// è¨ˆç®—ååé‡
			elapsed := time.Since(startTime).Seconds()
			throughput := float64(totalProcessed) / elapsed

			// è¨ˆç®— P99 å»¶é² (è¿”å›å…©å€‹å€¼: float64, error)
			p99Latency, err := stats.Percentile(latencies, 99)
			if err != nil {
				p99Latency = 0.0
			}

			// *** ä¿®æ­£é–‹å§‹ ***
			// è¨ˆç®—å¹³å‡å»¶é² (è¿”å›å…©å€‹å€¼: float64, error)
			avgLatency, err := stats.Mean(latencies)
			if err != nil {
				avgLatency = 0.0 // å¦‚æœè¨ˆç®—å¤±æ•—ï¼Œå¹³å‡å€¼è¨­ç‚º 0
			}
			// *** ä¿®æ­£çµæŸ ***

			// è¼¸å‡ºçµæœ (ç¾åœ¨ä½¿ç”¨ä¿®æ­£å¾Œçš„ avgLatency è®Šæ•¸)
			log.Printf("Processed: %d msgs | Throughput: %.2f msg/s | Avg Latency: %.2f ms | P99 Latency: %.2f ms",
				totalProcessed,
				throughput,
				avgLatency*1000, // ä½¿ç”¨ avgLatency
				p99Latency*1000,
			)

			// é‡ç½®è¨ˆæ•¸å™¨ (ä¸é‡ç½®ç¸½è¨ˆæ•¸å’Œé–‹å§‹æ™‚é–“ï¼Œä»¥è¨ˆç®—ç¸½å¹³å‡)
			latencies = nil
		}
	}
	log.Println("Stats Aggregator finished final calculation and shut down.")
}

func main() {
	log.Println("ğŸš€ Go Consumer started... waiting for messages")

	// 1. è¨­ç½® Context å’Œä¿¡è™Ÿè™•ç†
	ctx, cancel := context.WithCancel(context.Background())
	sigChan := make(chan os.Signal, 1)
	// ç›£è½ Ctrl+C (SIGINT) å’Œ çµ‚æ­¢ä¿¡è™Ÿ (SIGTERM)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	// å•Ÿå‹•ä¸€å€‹ Goroutine ä¾†ç­‰å¾…çµæŸä¿¡è™Ÿ
	go func() {
		<-sigChan // é˜»å¡ç›´åˆ°æ”¶åˆ°ä¿¡è™Ÿ
		log.Println("Received termination signal (Ctrl+C). Cancelling Context...")
		cancel() // æ”¶åˆ°ä¿¡è™Ÿå¾Œï¼Œå–æ¶ˆä¸»è®€å–è¿´åœˆçš„ Context
	}()

	// 2. åˆå§‹åŒ– Kafka Reader
	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        []string{KAFKA_BROKER},
		Topic:          TOPIC,
		GroupID:        GROUP_ID,
		MinBytes:       10e3, // 10KB
		MaxBytes:       10e6, // 10MB
		CommitInterval: time.Second,
		StartOffset:    kafka.FirstOffset, // å¾æœ€æ—©çš„ offset é–‹å§‹è®€å–
	})
	// ç¢ºä¿åœ¨ main å‡½å¼çµæŸæ™‚é—œé–‰ Reader (ç„¡è«–æ˜¯æ­£å¸¸é‚„æ˜¯ç•°å¸¸é€€å‡º)
	defer r.Close()

	// 3. è¨­ç½®ä½µç™¼çµæ§‹
	messageChannel := make(chan []byte, 1000) // è¨Šæ¯ç·©è¡å€
	resultChannel := make(chan float64, 1000) // çµæœç·©è¡å€

	// å•Ÿå‹•çµ±è¨ˆ Goroutine (éé˜»å¡)
	go statsAggregator(resultChannel)

	// å•Ÿå‹• Worker Pool
	var wg sync.WaitGroup
	for i := 1; i <= WORKER_COUNT; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			worker(workerID, messageChannel, resultChannel)
		}(i)
	}

	// 4. --- ä¸»è®€å–è¿´åœˆ ---
	for {
		m, err := r.ReadMessage(ctx)
		if err != nil {
			// **** é—œéµä¿®æ­£åœ¨é€™è£¡ ****
			if errors.Is(err, context.Canceled) {
				// æ”¶åˆ° context canceled éŒ¯èª¤ï¼Œè¡¨ç¤ºæˆ‘å€‘ä¸»å‹•ç™¼å‡ºäº†é€€å‡ºä¿¡è™Ÿ
				log.Println("Kafka Reader Context cancelled. Exiting read loop...")
				break // é€€å‡ºä¸»è®€å–è¿´åœˆï¼Œé€²å…¥å„ªé›…é—œé–‰æµç¨‹
			}
			// è™•ç†å…¶ä»–å¯¦éš›çš„é€£ç·šéŒ¯èª¤
			log.Printf("Error reading message: %v", err)
			break
		}
		// å°‡è¨Šæ¯ç™¼é€åˆ° Worker Channel
		messageChannel <- m.Value
	}

	// 5. å„ªé›…é—œé–‰æµç¨‹
	log.Println("Waiting for workers to finish current batch...")

	// é—œé–‰ messageChannelï¼Œé€šçŸ¥æ‰€æœ‰ Worker å”ç¨‹ (range msgs) é€€å‡º
	close(messageChannel)

	// ç­‰å¾…æ‰€æœ‰ Worker å”ç¨‹é€é wg.Done() å®Œæˆå·¥ä½œä¸¦é€€å‡º
	wg.Wait()

	// é—œé–‰ resultChannelï¼Œé€šçŸ¥ statsAggregator å”ç¨‹ (range results) é€€å‡ºä¸¦è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
	close(resultChannel)

	// æœ€çµ‚é€€å‡º
	log.Println("All Goroutines finished. Go Consumer gracefully shut down.")
}
