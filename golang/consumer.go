package main

import (
	"context"
	"encoding/json"
	"errors"
	"log"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"github.com/montanaflynn/stats" // ç”¨æ–¼ P99 çµ±è¨ˆ
	"github.com/segmentio/kafka-go"
)

// è¨Šæ¯çµæ§‹ï¼Œå°æ‡‰ Producer ç™¼é€çš„ JSON
type SensorData struct {
	Timestamp float64 `json:"timestamp"` // Producer å‚³é€æ™‚é–“ (ç§’ç´š)
	DeviceID  string  `json:"device_id"`
	Value     float64 `json:"value"`
}

// --- é…ç½®åƒæ•¸ ---
const (
	KAFKA_BROKER = "localhost:9092"
	TOPIC        = "test-data"
	// ç§»é™¤ GROUP_IDï¼Œå› ç‚ºæˆ‘å€‘ç¾åœ¨æ˜¯æ‰‹å‹•è®€å– Partitionï¼Œä¸åƒèˆ‡ Consumer Group è‡ªå‹•åˆ†é…
	// GROUP_ID = "go-consumer"

	// WORKER_COUNT ç¾åœ¨ä½œç‚º Partition çš„æ•¸é‡ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ CPU Worker çš„æ•¸é‡
	WORKER_COUNT        = 8
	BATCH_SIZE          = 1000   // çµ±è¨ˆæ‰¹æ¬¡å¤§å°ï¼Œèˆ‡ Python æ¸¬è©¦çš„ 1000 ä¿æŒä¸€è‡´
	CHANNEL_BUFFER_SIZE = 100000 // ã€ä¿®æ­£ã€‘å°‡ç·©è¡å€å¤§å°å¢å¤§ï¼Œé˜²æ­¢ I/O é˜»å¡
)

// --- è™•ç†å™¨ (Worker) ---
// è² è²¬è§£æ JSONã€è¨ˆç®—å»¶é²ï¼Œä¸¦å°‡å»¶é²çµæœç™¼é€çµ¦çµ±è¨ˆå™¨
func worker(id int, msgs <-chan []byte, results chan<- float64) {
	var data SensorData
	for msgBytes := range msgs {
		recvTimeNano := time.Now().UnixNano() // Go æ¥æ”¶æ™‚é–“ (ç´ç§’)

		// 1. JSON ååºåˆ—åŒ– (CPU å¯†é›†)
		if err := json.Unmarshal(msgBytes, &data); err != nil {
			log.Printf("Worker %d: JSON Unmarshal error: %v", id, err)
			continue
		}

		// 2. è¨ˆç®—å»¶é² (Latency)
		// å°‡ Go çš„æ¥æ”¶æ™‚é–“ (ç´ç§’) è½‰æ›ç‚ºç§’ï¼Œç„¶å¾Œè¨ˆç®—å»¶é²
		recvTimeSec := float64(recvTimeNano) / float64(time.Second)
		latency := recvTimeSec - data.Timestamp

		// 3. ç™¼é€çµæœ
		results <- latency
	}
	log.Printf("Worker %d: Exiting...", id)
}

// --- çµ±è¨ˆå™¨ (Stats Aggregator) ---
// è² è²¬å½™ç¸½å»¶é²ã€è¨ˆç®—ååé‡ã€P99 å»¶é²ä¸¦è¼¸å‡º
func statsAggregator(results <-chan float64) {
	var (
		latencies      []float64
		totalProcessed int64
		startTime      = time.Now().UnixNano() // ç¨‹å¼å•Ÿå‹•æ™‚é–“
	)

	for latency := range results {
		latencies = append(latencies, latency)
		totalProcessed++

		// æ¯ BATCH_SIZE è¼¸å‡ºä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
		if totalProcessed%BATCH_SIZE == 0 {
			// å°‡ç´ç§’è½‰æ›ç‚ºç§’
			currentTime := time.Now().UnixNano()
			elapsedTime := float64(currentTime-startTime) / float64(time.Second)

			// è¨ˆç®—ååé‡
			throughput := float64(totalProcessed) / elapsedTime

			// è¨ˆç®—å¹³å‡å»¶é² (Average Latency)
			avgLatency := 0.0
			for _, l := range latencies {
				avgLatency += l
			}
			avgLatency /= float64(len(latencies))

			// è¨ˆç®— P99 å»¶é²
			p99Latency, err := stats.Percentile(latencies, 99)
			if err != nil {
				p99Latency = 0.0 // éŒ¯èª¤è™•ç†
			}

			// è¼¸å‡ºçµæœ (èˆ‡æ‚¨çš„æ ¼å¼ä¿æŒä¸€è‡´)
			log.Printf("Processed: %d msgs | Throughput: %.2f msg/s | Avg Latency: %.2f ms | P99 Latency: %.2f ms",
				totalProcessed,
				throughput,
				avgLatency*1000,
				p99Latency*1000)

			// é‡è¨­ç‹€æ…‹
			latencies = nil // æ¸…ç©º latencies
		}
	}
	log.Println("Stats Aggregator: Exiting...")
}

// ã€æ–°å¢/ä¿®æ­£ã€‘é‡å°å–®ä¸€ Partition çš„è®€å– Goroutine (I/O ä½µè¡ŒåŒ–)
func partitionReader(ctx context.Context, partitionID int, messageChannel chan<- []byte, wg *sync.WaitGroup) {
	defer wg.Done()

	// 1. åˆå§‹åŒ– Partition å°ˆå±¬çš„ Kafka Reader
	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers: []string{KAFKA_BROKER},
		Topic:   TOPIC,
		// ğŸ”´ é—œéµä¿®æ­£: è®€å–æŒ‡å®š Partition æ™‚ï¼Œä¸è¨­ç½® GROUP_IDï¼Œæ‰‹å‹•æŒ‡å®š Partition
		Partition:   partitionID,
		StartOffset: kafka.FirstOffset,

		MinBytes: 10e3, // 10KB (æ‰¹æ¬¡è®€å–è¨­å®š)
		MaxBytes: 10e6, // 10MB
		// ã€ä¿®æ­£ã€‘å°‡ MaxWait é™ä½ï¼Œè®“ I/O å‘¼å«æ›´ç©æ¥µ
		MaxWait: 100 * time.Millisecond,
	})
	defer r.Close()

	log.Printf("Partition Reader %d: Started reading from Partition %d...", partitionID, partitionID)

	// 2. ä¸»è®€å–è¿´åœˆ
	for {
		m, err := r.ReadMessage(ctx)
		if err != nil {
			if errors.Is(err, context.Canceled) {
				log.Printf("Partition Reader %d: Context cancelled. Exiting read loop...", partitionID)
				return // é€€å‡º Goroutine
			}
			// é¿å…å› æš«æ™‚æ€§éŒ¯èª¤ (å¦‚é€£ç·šä¸­æ–·) è€Œå°è‡´ç¨‹å¼é€€å‡º
			log.Printf("Partition Reader %d: Error reading message: %v", partitionID, err)
			time.Sleep(time.Second)
			continue
		}

		// 3. å°‡è¨Šæ¯ç™¼é€åˆ° Worker Channel (Channel ç·©è¡å€å¾ˆå¤§ï¼Œé€™è£¡å¹¾ä¹ä¸æœƒé˜»å¡)
		messageChannel <- m.Value
	}
}

func main() {
	log.Println("ğŸš€ Go Consumer started... waiting for messages")

	// 1. è¨­ç½® Context å’Œä¿¡è™Ÿè™•ç†
	ctx, cancel := context.WithCancel(context.Background())
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigChan
		log.Println("Received termination signal (Ctrl+C). Cancelling Context...")
		cancel() // è§¸ç™¼æ‰€æœ‰ Goroutine é€€å‡º
	}()

	// 2. è¨­ç½®ä½µç™¼çµæ§‹
	messageChannel := make(chan []byte, CHANNEL_BUFFER_SIZE) // è¨Šæ¯ç·©è¡å€
	resultChannel := make(chan float64, CHANNEL_BUFFER_SIZE) // çµæœç·©è¡å€

	// å•Ÿå‹•çµ±è¨ˆ Goroutine
	go statsAggregator(resultChannel)

	// 3. å•Ÿå‹• Worker Pool (CPU å¯†é›†è™•ç†): 8 å€‹ Worker
	var wgWorker sync.WaitGroup
	for i := 1; i <= WORKER_COUNT; i++ {
		wgWorker.Add(1)
		go func(workerID int) {
			defer wgWorker.Done()
			worker(workerID, messageChannel, resultChannel)
		}(i)
	}

	// 4. ğŸš€ å•Ÿå‹• 8 å€‹ Partition Reader Goroutine (I/O å¯†é›†è®€å–)
	var wgReader sync.WaitGroup
	// å‡è¨­ Partition ID å¾ 0 é–‹å§‹åˆ° 7 (å…± 8 å€‹)
	for i := 0; i < WORKER_COUNT; i++ {
		wgReader.Add(1)
		// å‚³é wgReader å’Œ ctx
		go partitionReader(ctx, i, messageChannel, &wgReader)
	}

	// 5. ç­‰å¾…æ‰€æœ‰ Reader çµæŸ (ç•¶ä¿¡è™Ÿè§¸ç™¼ cancel æ™‚)
	wgReader.Wait()

	// 6. å„ªé›…é—œé–‰æµç¨‹
	log.Println("All Partition Readers shut down. Waiting for workers to finish current batch...")

	// é—œé–‰ messageChannelï¼Œé€šçŸ¥æ‰€æœ‰ Worker å”ç¨‹ (range msgs) é€€å‡º
	close(messageChannel)

	// ç­‰å¾…æ‰€æœ‰ Worker å”ç¨‹å®Œæˆå·¥ä½œä¸¦é€€å‡º
	wgWorker.Wait()

	// é—œé–‰ resultChannelï¼Œé€šçŸ¥ statsAggregator å”ç¨‹é€€å‡º
	close(resultChannel)

	log.Println("All Goroutines finished. Go Consumer gracefully shut down.")
}
