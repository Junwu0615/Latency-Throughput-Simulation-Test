package main

import (
	"context"
	"encoding/json"
	"log"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"github.com/segmentio/kafka-go"
)

// è¨Šæ¯çµæ§‹ï¼Œå°æ‡‰ Producer ç™¼é€çš„ JSON
type SensorData struct {
	Timestamp float64 `json:"timestamp"` // Producer å‚³é€æ™‚é–“ (ç§’ç´š)
	DeviceID  string  `json:"device_id"`
	Value     float64 `json:"value"`
}

// çµ±è¨ˆçµæ§‹
type Stats struct {
	Latencies      []float64
	TotalProcessed int64
	StartTime      float64
}

// --- é…ç½®åƒæ•¸ ---
const (
	KAFKA_BROKER = "localhost:9092"
	TOPIC        = "test-data"
	GROUP_ID     = "go-consumer"
	WORKER_COUNT = 8    // 8 å€‹ Goroutine è² è²¬ JSON è§£æå’Œè¨ˆç®—
	BATCH_SIZE   = 1000 // çµ±è¨ˆæ‰¹æ¬¡å¤§å°ï¼Œèˆ‡ Python æ¸¬è©¦çš„ 1000 ä¿æŒä¸€è‡´
)

// --- è™•ç†å™¨ (Worker) ---
func worker(id int, msgs <-chan []byte, results chan<- float64) {
	var data SensorData
	for msgBytes := range msgs {
		recvTime := time.Now().UnixNano() // æ¥æ”¶æ™‚é–“ (ç´ç§’)

		// 1. JSON ååºåˆ—åŒ– (CPU å¯†é›†)
		if err := json.Unmarshal(msgBytes, &data); err != nil {
			log.Printf("Worker %d: JSON Unmarshal error: %v", id, err)
			continue
		}

		// 2. å»¶é²è¨ˆç®— (CPU å¯†é›†)
		// å°‡ Producer çš„ float64 ç§’ç´šæ™‚é–“æˆ³è½‰æ›ç‚ºç´ç§’ç´šï¼Œä»¥ä¾¿ç²¾ç¢ºè¨ˆç®—
		producerNano := int64(data.Timestamp * 1e9)
		latencyNano := recvTime - producerNano
		latencySeconds := float64(latencyNano) / 1e9

		// å°‡å»¶é²çµæœå‚³é€çµ¦çµ±è¨ˆ Goroutine
		results <- latencySeconds
	}
}

// --- çµ±è¨ˆ Goroutine ---
func statsAggregator(results <-chan float64) {
	stats := Stats{
		Latencies:      make([]float64, 0, BATCH_SIZE),
		TotalProcessed: 0,
		StartTime:      float64(time.Now().UnixNano()),
	}

	ticker := time.NewTicker(5 * time.Second) // é¿å…çµ±è¨ˆéæ–¼é »ç¹
	defer ticker.Stop()

	for {
		select {
		case latency := <-results:
			stats.Latencies = append(stats.Latencies, latency)
			stats.TotalProcessed++

			// æ¯è™•ç† BATCH_SIZE ç­†è¨Šæ¯ï¼Œè¼¸å‡ºä¸€æ¬¡çµ±è¨ˆè³‡è¨Š (èˆ‡ Python å°ç­‰)
			if len(stats.Latencies) >= BATCH_SIZE {
				currentCount := stats.TotalProcessed
				elapsed := (float64(time.Now().UnixNano()) - stats.StartTime) / 1e9
				throughput := float64(currentCount) / elapsed

				// è¨ˆç®—çµ±è¨ˆæ•¸æ“š
				var sum float64
				for _, l := range stats.Latencies {
					sum += l
				}
				avgLatency := sum / float64(len(stats.Latencies))

				// P99 éœ€è¦æ’åºï¼Œç”±æ–¼ Go æ²’æœ‰å…§å»º P99ï¼Œé€™è£¡åªè¨ˆç®—å¹³å‡å»¶é²ï¼Œ
				// P99 å»ºè­°ä½¿ç”¨å¤–éƒ¨åº«æˆ–é‹è¡Œ Python è…³æœ¬ä¾†è¨ˆç®—ï¼Œ
				// ä½†ç‚ºäº†èˆ‡ Python æ¸¬è©¦é‚è¼¯å°ç­‰ï¼Œæˆ‘å€‘å¿…é ˆåŠ å…¥æ’åºè¨ˆç®—ã€‚
				// ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å…ˆåªè¼¸å‡ºå¹³å‡å»¶é²ï¼Œå› ç‚ºæ ¸å¿ƒæ¯”è¼ƒæ˜¯ååé‡ã€‚

				log.Printf("Processed: %d msgs | Throughput: %.2f msg/s | Avg Latency: %.2f ms",
					currentCount, throughput, avgLatency*1000)

				// æ¸…ç©º latencies åˆ—è¡¨
				stats.Latencies = stats.Latencies[:0]
			}

		case <-ticker.C:
			// é€±æœŸæ€§è¼¸å‡º (å¯é¸ï¼Œé¿å…é•·æ™‚é–“æ²’æœ‰æ»¿ BATCH_SIZE)
		}
	}
}

// --- ä¸»å‡½å¼ ---
func main() {
	log.SetFlags(log.Ldate | log.Ltime | log.Lmicroseconds)
	log.Println("ğŸš€ Go Consumer started... waiting for messages")

	// åˆå§‹åŒ– Kafka Reader
	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        []string{KAFKA_BROKER},
		Topic:          TOPIC,
		GroupID:        GROUP_ID,
		MinBytes:       10e3, // 10KB
		MaxBytes:       10e6, // 10MB
		CommitInterval: time.Second,
		StartOffset:    kafka.FirstOffset, // å¾æœ€æ—©çš„ offset é–‹å§‹è®€å–
	})
	defer r.Close()

	// å»ºç«‹ Channel
	messageChannel := make(chan []byte, 1000) // è¨Šæ¯ç·©è¡å€
	resultChannel := make(chan float64, 1000) // çµæœç·©è¡å€

	// å•Ÿå‹•çµ±è¨ˆ Goroutine
	go statsAggregator(resultChannel)

	// å•Ÿå‹• Worker Pool
	var wg sync.WaitGroup
	for i := 1; i <= WORKER_COUNT; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			worker(workerID, messageChannel, resultChannel)
		}(i)
	}

	// --- ä¸»è®€å–è¿´åœˆ ---
	ctx, cancel := context.WithCancel(context.Background())
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigChan
		cancel()
	}()

	// Kafka è®€å– (I/O é˜»å¡)
	for {
		m, err := r.ReadMessage(ctx)
		if err != nil {
			if err == context.Canceled {
				break // æ”¶åˆ°åœæ­¢è¨Šè™Ÿï¼Œé€€å‡ºè¿´åœˆ
			}
			log.Printf("Error reading message: %v", err)
			continue
		}

		// å°‡è¨Šæ¯å…§å®¹å‚³é€åˆ° Channel
		select {
		case messageChannel <- m.Value:
			// æˆåŠŸå‚³é€
		case <-ctx.Done():
			return
		default:
			// Channel å·²æ»¿ï¼Œç•¥éè¨Šæ¯ (æµé‡éå¤§æ™‚çš„èƒŒå£“è™•ç†)
		}
	}

	// é—œé–‰ Channel
	close(messageChannel)
	wg.Wait()
	// close(resultChannel) // çµ±è¨ˆ Goroutine ä»æœƒæŒçºŒé‹è¡Œï¼Œä½† worker å·²é€€å‡º

	log.Println("âœ… Go Consumer closed.")
}
