import time, json, statistics, redis
import numpy as np
from kafka import KafkaConsumer
from pymongo import MongoClient
from utils.logger import Logger


# TODO åˆå§‹åŒ– logger
logger = Logger(console_name=f'.consumer_console',
                file_name=f'.consumer_file')


# TODO åˆå§‹åŒ– Kafka Consumer
KAFKA_BROKER = 'localhost:9092'
TOPIC = 'test-data'
consumer = KafkaConsumer(
    TOPIC,
    bootstrap_servers=[KAFKA_BROKER],
    # auto_offset_reset='earliest',
    auto_offset_reset='latest',
    enable_auto_commit=True,
    group_id='python-consumer',
    value_deserializer=lambda v: json.loads(v.decode('utf-8'))
)


# TODO åˆå§‹åŒ– Redis
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_PASSWORD = 'redis_pass'

redis_client = None

try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        db=0
    )
    redis_client.ping()
    logger.warning('Redis æ¸¬è©¦é€£æ¥æˆåŠŸä¸¦é€šéé©—è­‰ï¼')

except redis.exceptions.AuthenticationError as e:
    logger.error('é©—è­‰å¤±æ•—ï¼šè«‹æª¢æŸ¥å¯†ç¢¼æ˜¯å¦æ­£ç¢º')

except Exception as e:
    logger.error('Redis é€£æ¥æˆ–æ“ä½œå¤±æ•—')


# TODO åˆå§‹åŒ– MongoDB
MONGO_USERNAME = 'mongo_user'
MONGO_PASSWORD = 'mongo_pass'
MONGO_HOST = 'localhost:27017'
MONGO_URI = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}/"

mongo_client = MongoClient(MONGO_URI)
db = mongo_client['perf_test'] # æ‡‰ç”¨ç¨‹å¼è³‡æ–™åº«

try:
    db.command('ping')
    logger.warning("MongoDB é€£æ¥ä¸¦é©—è­‰æˆåŠŸï¼")

except Exception as e:
    logger.error('MongoDB é€£æ¥æˆ–é©—è­‰å¤±æ•—')

collection = db['records']


# TODO æ¶ˆè²» Kafka è¨Šæ¯ä¸¦å¯«å…¥ Redis å’Œ MongoDB
latencies = []
count = 0
start_time = time.time()

# TODO: å‰µå»ºä¸€å€‹åˆ—è¡¨ä¾†æ”¶é›†æ•¸æ“š
redis_batch_data = []
mongo_batch_data = []
BATCH_SIZE = 1000 # æ‰¹æ¬¡å¤§å°

logger.warning('ğŸ”¥ Consumer started... waiting for messages')
try:
    for message in consumer:
        # è§£æ Kafka è¨Šæ¯
        data = message.value

        # è¨ˆç®—å»¶é²
        recv_time = time.time()
        latency = recv_time - data['timestamp']
        latencies.append(latency)

        # TODO Redis å¯«å…¥æ–¹å¼
        # --------- åŒæ­¥ ---------
        # redis_client.set(data['device_id'], json.dumps(data))
        # --------- ç•°æ­¥ ---------
        redis_batch_data.append((data['device_id'], json.dumps(data)))

        # TODO MongoDB å¯«å…¥æ–¹å¼
        # --------- åŒæ­¥ ---------
        # collection.insert_one({
        #     'device_id': data['device_id'],
        #     'value': data['value'],
        #     'producer_ts': data['timestamp'],
        #     'consumer_ts': recv_time,
        #     'latency': latency
        # })
        # --------- ç•°æ­¥ ---------
        mongo_batch_data.append({
            'device_id': data['device_id'],
            'value': data['value'],
            'producer_ts': data['timestamp'],
            'consumer_ts': recv_time,
            'latency': latency
        })

        count += 1

        # æ‰¹æ¬¡è™•ç†é‚è¼¯
        if count > 0 and count % BATCH_SIZE == 0:
            # TODO Redis Pipeline åŸ·è¡Œç•°æ­¥ SET
            pipe = redis_client.pipeline()
            for key, value in redis_batch_data:
                pipe.set(key, value)
            pipe.execute()
            redis_batch_data.clear()

            # TODO MongoDB insert_many åŸ·è¡Œç•°æ­¥æ’å…¥
            collection.insert_many(mongo_batch_data)
            mongo_batch_data.clear()

        # æ¯è™•ç† 1000 ç­†è¨Šæ¯ï¼Œè¼¸å‡ºä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
        if count % 1000 == 0:
            elapsed = time.time() - start_time
            throughput = count / elapsed
            avg_latency = statistics.mean(latencies)
            p99_latency = np.percentile(latencies, 99)

            logger.info(f'Processed: {count} msgs | '
                        f'Throughput: {throughput:.2f} msg/s | '
                        f'Avg Latency: {avg_latency * 1000:.2f} ms ( {avg_latency:.2f} s ) | '
                        f'P99 Latency: {p99_latency * 1000:.2f} ms ( {p99_latency:.2f} s )')

            latencies.clear() # æ¸…ç©º latencies åˆ—è¡¨ä»¥é‡æ–°é–‹å§‹ä¸‹ä¸€æ‰¹æ¬¡çš„çµ±è¨ˆ

except KeyboardInterrupt:
    try:
        logger.error('æ­£åœ¨é—œé–‰ Kafka Consumer ...', exc_info=False)
        consumer.close()
        logger.warning('Kafka Consumer å·²é—œé–‰ ...')

    except Exception as e:
        logger.error('é—œé–‰ Kafka Consumer æ™‚ç™¼ç”ŸéŒ¯èª¤')

    try:
        logger.error('æ­£åœ¨é—œé–‰ MongoDB é€£ç·š ...', exc_info=False)
        mongo_client.close()
        logger.warning('MongoDB é€£ç·šå·²é—œé–‰ !')

    except Exception as e:
        logger.error('é—œé–‰ MongoDB é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤')

    logger.error('æ­£åœ¨é—œé–‰ Redis é€£ç·š ...', exc_info=False)
    if 'redis_client' in locals() and isinstance(redis_client, redis.Redis):
        try:
            redis_client.connection_pool.disconnect()
            logger.warning("Redis é€£ç·šæ± å·²æ˜ç¢ºé—œé–‰ä¸¦é‡‹æ”¾è³‡æº !")

        except Exception as e:
            logger.error('é—œé–‰ Redis é€£ç·šæ± æ™‚ç™¼ç”ŸéŒ¯èª¤')