"""
TODO
    ååé‡ v2: å°å…¥ ThreadPoolExecutor # å¤šåŸ·è¡Œç·’
        - ç”¨å¤šå€‹åŸ·è¡Œç·’ä¾†åŒæ™‚åŸ·è¡Œ Redis å’Œ MongoDB çš„ I/O æ‰¹æ¬¡å¯«å…¥
    result:
        - Processed: ... msgs
        - Throughput: ... msg/s
        - Avg Latency: ... ms ( ... s )
        - P99 Latency: ... ms ( ... s )
"""
import time, json, statistics, redis
import numpy as np
from kafka import KafkaConsumer
from pymongo import MongoClient
from concurrent.futures import ThreadPoolExecutor
from utils.logger import Logger


# TODO åˆå§‹åŒ– logger
logger = Logger(console_name=f'.consumer_console',
                file_name=f'.consumer_file')


# TODO åˆå§‹åŒ– Kafka Consumer
KAFKA_BROKER = 'localhost:9092'
TOPIC = 'test-data'
consumer = KafkaConsumer(
    TOPIC,
    bootstrap_servers=[KAFKA_BROKER],
    # auto_offset_reset='earliest',
    auto_offset_reset='latest',
    enable_auto_commit=True,
    group_id='python-consumer',
    value_deserializer=lambda v: json.loads(v.decode('utf-8'))
)


# TODO åˆå§‹åŒ– Redis
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_PASSWORD = 'redis_pass'

redis_client = None

try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        db=0
    )
    redis_client.ping()
    logger.warning('Redis æ¸¬è©¦é€£æ¥æˆåŠŸä¸¦é€šéé©—è­‰ï¼')

except redis.exceptions.AuthenticationError as e:
    logger.error('é©—è­‰å¤±æ•—ï¼šè«‹æª¢æŸ¥å¯†ç¢¼æ˜¯å¦æ­£ç¢º')

except Exception as e:
    logger.error('Redis é€£æ¥æˆ–æ“ä½œå¤±æ•—')


# TODO åˆå§‹åŒ– MongoDB
MONGO_USERNAME = 'mongo_user'
MONGO_PASSWORD = 'mongo_pass'
MONGO_HOST = 'localhost:27017'
MONGO_URI = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}/"

mongo_client = MongoClient(MONGO_URI)
db = mongo_client['perf_test'] # æ‡‰ç”¨ç¨‹å¼è³‡æ–™åº«

try:
    db.command('ping')
    logger.warning("MongoDB é€£æ¥ä¸¦é©—è­‰æˆåŠŸï¼")

except Exception as e:
    logger.error('MongoDB é€£æ¥æˆ–é©—è­‰å¤±æ•—')

collection = db['records']


# TODO æ¶ˆè²» Kafka è¨Šæ¯ä¸¦å¯«å…¥ Redis å’Œ MongoDB
latencies = []
count = 0
start_time = time.time()

# TODO: å‰µå»ºä¸€å€‹åˆ—è¡¨ä¾†æ”¶é›†æ•¸æ“š
redis_batch_data = []
mongo_batch_data = []
BATCH_SIZE = 1000 # æ‰¹æ¬¡å¤§å°

# TODO
#  åˆå§‹åŒ– I/O åŸ·è¡Œç·’æ±  ( 2 å€‹åŸ·è¡Œç·’å°ˆé–€è² è²¬ Redis å’Œ Mongo å¯«å…¥ )
#  ä¸»åŸ·è¡Œç·’å¯ç¹¼çºŒè™•ç† Kafka è¨Šæ¯
io_executor = ThreadPoolExecutor(max_workers=2)


def write_to_redis(data_to_write):
    # TODO Redis Pipeline åŸ·è¡Œç•°æ­¥ SET
    pipe = redis_client.pipeline()
    for key, value in data_to_write:
        pipe.set(key, value)
    pipe.execute()
    redis_batch_data.clear()
    # logger.info('Redis Batch Write Complete')


def write_to_mongo(data_to_write):
    # TODO MongoDB insert_many åŸ·è¡Œç•°æ­¥æ’å…¥
    collection.insert_many(data_to_write)
    mongo_batch_data.clear()
    # logger.info('MongoDB Batch Write Complete')


logger.warning('ğŸ”¥ Consumer started... waiting for messages')
try:
    for message in consumer:
        # è§£æ Kafka è¨Šæ¯
        data = message.value

        # è¨ˆç®—å»¶é²
        recv_time = time.time()
        latency = recv_time - data['timestamp']
        latencies.append(latency)

        # TODO Redis å¯«å…¥æ–¹å¼
        # --------- ç•°æ­¥ ---------
        redis_batch_data.append((data['device_id'], json.dumps(data)))

        # TODO MongoDB å¯«å…¥æ–¹å¼
        # --------- ç•°æ­¥ ---------
        mongo_batch_data.append({
            'device_id': data['device_id'],
            'value': data['value'],
            'producer_ts': data['timestamp'],
            'consumer_ts': recv_time,
            'latency': latency
        })

        count += 1

        # æ‰¹æ¬¡è™•ç†é‚è¼¯
        if count > 0 and count % BATCH_SIZE == 0:
            # è¤‡è£½ç•¶å‰çš„ç·©è¡å€æ•¸æ“šï¼Œé¿å…åœ¨åŸ·è¡Œç·’ä¸­è¢«ä¿®æ”¹
            redis_data_copy = redis_batch_data.copy()
            mongo_data_copy = mongo_batch_data.copy()

            # æ¸…ç©ºä¸»åŸ·è¡Œç·’çš„ç·©è¡å€ï¼Œæº–å‚™æ¥æ”¶ä¸‹ä¸€æ‰¹æ•¸æ“š
            redis_batch_data.clear()
            mongo_batch_data.clear()

            # æäº¤ä»»å‹™çµ¦åŸ·è¡Œç·’æ±  (éé˜»å¡ï¼Œä¸»åŸ·è¡Œç·’ç«‹å³è¿”å›)
            io_executor.submit(write_to_redis, redis_data_copy)
            io_executor.submit(write_to_mongo, mongo_data_copy)


        # æ¯è™•ç† 1000 ç­†è¨Šæ¯ï¼Œè¼¸å‡ºä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
        if count % 1000 == 0:
            elapsed = time.time() - start_time
            throughput = count / elapsed
            avg_latency = statistics.mean(latencies)
            p99_latency = np.percentile(latencies, 99)

            logger.info(f'Processed: {count} msgs | '
                        f'Throughput: {throughput:.2f} msg/s | '
                        f'Avg Latency: {avg_latency * 1000:.2f} ms ( {avg_latency:.2f} s ) | '
                        f'P99 Latency: {p99_latency * 1000:.2f} ms ( {p99_latency:.2f} s )')

            latencies.clear() # æ¸…ç©º latencies åˆ—è¡¨ä»¥é‡æ–°é–‹å§‹ä¸‹ä¸€æ‰¹æ¬¡çš„çµ±è¨ˆ

except KeyboardInterrupt:
    try:
        logger.error('æ­£åœ¨é—œé–‰ Kafka Consumer ...', exc_info=False)
        consumer.close()
        logger.warning('Kafka Consumer å·²é—œé–‰ !')

    except Exception as e:
        logger.error('é—œé–‰ Kafka Consumer æ™‚ç™¼ç”ŸéŒ¯èª¤')

    try:
        logger.error('æ­£åœ¨é—œé–‰ MongoDB é€£ç·š ...', exc_info=False)
        mongo_client.close()
        logger.warning('MongoDB é€£ç·šå·²é—œé–‰ !')

    except Exception as e:
        logger.error('é—œé–‰ MongoDB é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤')

    logger.error('æ­£åœ¨é—œé–‰ Redis é€£ç·š ...', exc_info=False)
    if 'redis_client' in locals() and isinstance(redis_client, redis.Redis):
        try:
            redis_client.connection_pool.disconnect()
            logger.warning('Redis é€£ç·šæ± å·²æ˜ç¢ºé—œé–‰ä¸¦é‡‹æ”¾è³‡æº !')

        except Exception as e:
            logger.error('é—œé–‰ Redis é€£ç·šæ± æ™‚ç™¼ç”ŸéŒ¯èª¤')